# -*- coding: utf-8 -*-
"""EDAandMLalgo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Vtr0SYcIbNcycEPPZfKDw4l7vtqMB3_
"""

import pandas as pd

df=pd.read_csv('/content/sample_data/train_IxoE5JN.csv')

df.isnull().sum()

df1=df.ffill().bfill()

df1.drop('row_id',axis=1,inplace=True)

df1

df1['Date']=df1['datetime'].str.split('-').str[2]
df1['Month']=df1['datetime'].str.split('-').str[1]
df1['Year']=df1['datetime'].str.split('-').str[0]

df1

df1['date']=df1['Date'].str.split(' ').str[0]

df1

df1['Date']=df1['Date'].str.split(' ').str[1]

df1

df1['time']=df1['Date'].str.split(':').str[0]

df1

df1['date']=df1['date'].astype(int)
df1['Month']=df1['Month'].astype(int)
df1['Year']=df1['Year'].astype(int)
df1['time']=df1['time'].astype(int)

df1











df1.drop(columns = ["datetime","Date"],inplace=True,axis=1)

df1.drop(columns = ["Month"],inplace=True,axis=1)

df1.columns

X=df1.drop(labels=['energy'],axis=1)  #Target and feature split
y=df1['energy']

from sklearn.model_selection import train_test_split, GridSearchCV

#Module related to calculation of metrics
from sklearn import metrics

from sklearn.preprocessing import StandardScaler, OneHotEncoder

import statsmodels.formula.api as smf

from sklearn.metrics import accuracy_score, confusion_matrix,classification_report

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR
from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB
from sklearn.ensemble import GradientBoostingRegressor
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import OneHotEncoder,PowerTransformer, StandardScaler, MinMaxScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge,Lasso
from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error
from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV
import pickle
from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from xgboost import XGBRegressor
from sklearn.svm import SVR
from math import sqrt

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=256)

#RANDOM FOREST
pipe = Pipeline([
                ('model',
                 RandomForestRegressor(max_depth=40, max_features=3,
                                       min_samples_leaf=3, min_samples_split=8,
                                       n_estimators=200))])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

#SAVING MODEL
import pickle
pickle.dump(pipe, open('model.pkl', 'wb'))
pickled_model = pickle.load(open('model.pkl', 'rb'))

#XGBOOST
pipe = Pipeline([
        ('model',XGBRegressor(colsample_bytree=1.0, gamma=0.5, max_depth=5, subsample=0.8))])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

#MODEL SAVING
import pickle
pickle.dump(pipe, open('modelXG.pkl', 'wb'))
pickled_model = pickle.load(open('modelXG.pkl', 'rb'))

#CATBOOST
pipe = Pipeline([
        ('model',CatBoostRegressor(depth= 10, iterations=200, learning_rate= 0.1))
    ])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

import pickle
pickle.dump(pipe, open('modelCAT.pkl', 'wb'))
pickled_model = pickle.load(open('modelCAT.pkl', 'rb'))

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3, random_state=198)
pipe = Pipeline([
        ('model',SVR())
    ])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))



import pickle
pickle.dump(pipe, open('modelSVR.pkl', 'wb'))
pickled_model = pickle.load(open('modelSVR.pkl', 'rb'))

pipe = Pipeline([
        ('model',GradientBoostingRegressor(criterion='squared_error',
                          learning_rate=0.176655593656432, max_depth=4,
                          max_leaf_nodes=50, n_estimators=200))   ])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

import pickle
pickle.dump(pipe, open('modelGRAD.pkl', 'wb'))
pickled_model = pickle.load(open('modelGRAD.pkl', 'rb'))

import lightgbm as lgb
gbm = lgb.LGBMRegressor(objective='regression',num_leaves=100,learning_rate=0.2,n_estimators=200)
gbm.fit(X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='l2_root',
    early_stopping_rounds=10)
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

import pickle
pickle.dump(gbm, open('modelgbm.pkl', 'wb'))
pickled_model = pickle.load(open('modelgbm.pkl', 'rb'))

tdf1=tdf1.drop(['Year_2019','Year_2020','Year_2021'],axis=1)



import xgboost as xgb
boost_params = {'eval_metric': 'rmse'}
xgb0 = xgb.XGBRegressor(
max_depth=8,
learning_rate=0.1,
n_estimators=200,
objective='reg:linear',
gamma=0,
min_child_weight=1,
subsample=1,
colsample_bytree=1,
scale_pos_weight=1,
seed=27,
**boost_params)
xgb0.fit(X_train,y_train)
accuracyxgboost = round(xgb0.score(X_train, y_train)*100,2)
predict_xgboost = xgb0.predict(X_test)
msexgboost = mean_squared_error(y_test,predict_xgboost)
rmsexgboost= np.sqrt(msexgboost)

rmsexgboost

"""# TEST"""

tdf=pd.read_csv('/content/sample_data/test_WudNWDM.csv')

tdf1=tdf.ffill().bfill()

tdf1.drop('row_id',axis=1,inplace=True)

tdf1['Date']=tdf1['datetime'].str.split('-').str[2]
tdf1['Month']=tdf1['datetime'].str.split('-').str[1]
tdf1['Year']=tdf1['datetime'].str.split('-').str[0]

tdf1['date']=tdf1['Date'].str.split(' ').str[0]

tdf1['Date']=tdf1['Date'].str.split(' ').str[1]

tdf1['time']=tdf1['Date'].str.split(':').str[0]

tdf1['date']=tdf1['date'].astype(int)
tdf1['Month']=tdf1['Month'].astype(int)
tdf1['Year']=tdf1['Year'].astype(int)
tdf1['time']=tdf1['time'].astype(int)

tdf1.drop(columns = ["datetime","Date"],inplace=True,axis=1)

tdf1









tdf2=sc.transform(tdf1)

pipe1 = Pipeline([
        ('scale',scale)])

tdf2=pipe1.transform(tdf1.values)

scaled_test_df=pd.DataFrame(data=tdf2, columns=tdf1.columns, index=tdf1.index)

tdf1

scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(array(df1['energy']).reshape(len(df1['energy']), 1))
series = pd.DataFrame(scaled)
series.columns = ['ener']

one_hot = pd.get_dummies(tdf1['Month'], prefix='Month')
tdf1 = tdf1.join(one_hot)



tdf1.drop(columns = ["Month","Year"],inplace=True,axis=1)

tdf1

tdf1

tdf1.replace({2019: 11, 2020:12 ,2021:13}, inplace = True) # label encoding

tdf1

y

pickled_model = pickle.load(open('modelgbm.pkl', 'rb'))
a=pickled_model.predict(tdf1)

from tensorflow import keras
model = keras.models.load_model('model.h5')

a=model.predict(tdf1)

a



a=pd.DataFrame(a)

d=pd.read_csv('/content/sample_data/test_WudNWDM.csv')

d=d[d.columns[0]]

d

d=pd.DataFrame(d)

b=d.join(a)

b

b.rename(columns = {0:'energy'}, inplace = True)

b

b.to_csv('final_output5.csv', index=False)



pipe = Pipeline([
    ('model',RandomForestRegressor(random_state=96,n_jobs=-1))
])
# Number of trees in random forest
n_estimators = [100, 200]
# Maximum number of levels in tree
max_depth = [80, 90, 100, 110]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [8, 10, 12]
# Minimum number of samples required at each leaf node
min_samples_leaf =[3, 4, 5]
# Method of selecting samples for training each tree
criterion =['rmse']
max_features=[2,3]
# Create the random grid
random_grid = {'model__n_estimators': n_estimators,
               'model__max_depth': max_depth,
               'model__min_samples_split': min_samples_split,
               'model__min_samples_leaf': min_samples_leaf,
               'model__max_features': max_features 
            }

gs = GridSearchCV(estimator=pipe, param_grid=random_grid, n_jobs=-1)

gs.fit(X_train, y_train)

gs.best_estimator_

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3, random_state=198)
pipe = Pipeline([
        ('scale',scale),
        ('model',XGBRegressor())
    ])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from xgboost import XGBRegressor

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3, random_state=198)
pipe = Pipeline([
        ('model',CatBoostRegressor('depth'= 10, 'iterations'=100, 'learning_rate'= 0.1))
    ])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))



from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from xgboost import XGBRegressor
# load the dataset

model = XGBRegressor()
# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)
# force scores to be positive
scores = absolute(scores)
print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )

y_pred = pipe.predict(X_test)
print('R2 SCORE', r2_score(y_test,y_pred))
print('RMSE', sqrt(mean_squared_error(y_test,y_pred)))

from catboost import CatBoostRegressor
from sklearn.model_selection import GridSearchCV

model = CatBoostRegressor()
parameters = {'depth' : [6,8,10,12,16],
              'learning_rate' : [0.01, 0.05, 0.1],
              'iterations'    : [30, 50, 100,150,200]
              }

grid = GridSearchCV(estimator=model, param_grid = parameters, cv = 2, n_jobs=-1)
grid.fit(X_train, y_train)

grid.best_params_

!pip install catboost

from sklearn.model_selection import GridSearchCV
import lightgbm as lgb

param_distributions = {
        #         "device_type": trial.suggest_categorical("device_type", ['gpu']),
        "n_estimators": [200,500],
        "learning_rate": [0.01, 0.1,0.3],
        "num_leaves": [10,20, 30],
        "max_depth": [3, 12],
        "min_data_in_leaf": [100, 200],
        "lambda_l1": [0, 50, 5],
        "lambda_l2": [0, 100, 5],
        "min_gain_to_split": [ 0, 15],
        "bagging_fraction":[0.2, 0.9,0.1],
        "feature_fraction":[0.2, 0.9,0.1],
    } 


grid = GridSearchCV(lgb(objective="binary"), param_grid = param_distributions)
grid.fit(X_train, y_train)

from scipy.stats import loguniform
from sklearn.ensemble import GradientBoostingRegressor

param_distributions = {
    "n_estimators": [1, 2, 5, 10, 20, 50, 100, 200, 500],
    "max_leaf_nodes": [2, 5, 10, 20, 50, 100],
    "learning_rate": loguniform(0.01, 1),
    "max_depth":[2,3,4,5,6,8,10],
    "criterion" :['friedman_mse', 'squared_error', 'mse']
}
search_cv = RandomizedSearchCV(
    GradientBoostingRegressor(), param_distributions=param_distributions,
    scoring="neg_mean_absolute_error", n_iter=20, random_state=0, n_jobs=2
)
search_cv.fit(X_train, y_train)

search_cv.best_estimator_

XGBRegressor()

from sklearn.model_selection import GridSearchCV
params = {
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }
search_cv = RandomizedSearchCV( XGBRegressor(), param_distributions=params,
    scoring="neg_mean_absolute_error", n_iter=20, random_state=0, n_jobs=2
)
search_cv.fit(X_train, y_train)

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, train_size=0.3, random_state=256)

"""# Linear reg"""

from sklearn.preprocessing import StandardScaler
sc =StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

X_train

X_test

X_train.shape

## Part 2 Now lets create the ANN
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ReLU
from tensorflow.keras.layers import Dropout

### Lets initialize the ANN
classifier=Sequential()

## Adding the input Layer and first hidden layer
classifier.add(Dense(10,input_shape=(15,),activation='relu'))

# adding the first hidden layer
classifier.add(Dense(units=10,activation='relu'))
#classifier.add(Dropout(0.2))

##adding the second hidden layer
classifier.add(Dense(units=10,activation='relu'))
#classifier.add(Dropout(0.3))

##  Adding the output layer
classifier.add(Dense(1, kernel_initializer='normal', activation='linear'))

classifier.summary()

### Lets initialize the ANN
regressor=Sequential()
## Adding the input Layer and first hidden layer
regressor.add(Dense(10,input_shape=(15,),activation='relu'))


# adding the first hidden layer
regressor.add(Dense(units=10,activation='relu'))
#regressor.add(Dropout(0.2))

##adding the second hidden layer
regressor.add(Dense(units=10,activation='relu'))
#regressor.add(Dropout(0.3))
regressor.add(Dense(units=10,activation='relu'))
regressor.add(Dense(units=10,activation='relu'))
##  Adding the output layer
regressor.add(Dense(1))

import tensorflow
mse=tensorflow.keras.losses.MeanSquaredError(
    name='mean_squared_error'
)
metrics=tensorflow.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)

import tensorflow
opt=tensorflow.keras.optimizers.Adam(learning_rate=0.017245)

regressor.compile(optimizer=opt,loss=mse,metrics=['mae'])

## Early Stopping
import tensorflow as tf
early_stopping=tf.keras.callbacks.EarlyStopping(
    monitor="mae",
    patience=10
   
)

model_history=regressor.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=10,epochs=50,callbacks=early_stopping)

y_test

y_pred=regressor.predict(X_test)

y_pred

print(r2_score(y_test,y_pred))





regressor.save('model.h5')



df1

one_hot = pd.get_dummies(df1['Month'], prefix='Month')
df1 = df1.join(one_hot)

df1

df1.replace({2008: 0,2009:1 ,2010:2,2011:3 ,2012:4,2013:5,2014:6,2015:7,2016:8,2017:9,2018:10}, inplace = True) # label encoding

df1

one_hot = pd.get_dummies(df1['Year'], prefix='Year')
df1 = df1.join(one_hot)

df1

scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(array(df1['energy']).reshape(len(df1['energy']), 1))
series = pd.DataFrame(scaled)
series.columns = ['ener']

import pandas as pd;
import matplotlib.pyplot as plt
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import RNN, SimpleRNN
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.layers.core import Activation
from keras.callbacks import LambdaCallback
from sklearn.preprocessing import MinMaxScaler

dataset = pd.merge(df1, series, left_index=True, right_index=True)

dataset

df1=dataset.drop(['energy'],axis=1)

df1

number_of_test_data = 50
number_of_holdout_data = 50
number_of_training_data = len(dataset) - number_of_holdout_data - number_of_test_data
print ("total, train, test, holdout:", len(dataset), number_of_training_data, number_of_test_data, number_of_holdout_data)

datatrain = dataset[:number_of_training_data]
datatest = dataset[-(number_of_test_data+number_of_holdout_data):-number_of_holdout_data]
datahold = dataset[-number_of_holdout_data:]

in_seq1 = array(datatrain['date'])
in_seq2 = array(datatrain['time'])
in_seq3 = array(datatrain['Month_1'])
in_seq4 = array(datatrain['Month_2'])
in_seq5 = array(datatrain['Month_3'])
in_seq6 = array(datatrain['Month_4'])
in_seq7 = array(datatrain['Month_5'])
in_seq8 = array(datatrain['Month_6'])
in_seq9 = array(datatrain['Month_7'])
in_seq10 = array(datatrain['Month_8'])
in_seq11 = array(datatrain['Month_9'])
in_seq12 = array(datatrain['Month_10'])
in_seq13 = array(datatrain['Month_11'])
in_seq14 = array(datatrain['Month_12'])
in_seq15 = array(datatrain['Year_2008'])
in_seq16 = array(datatrain['Year_2009'])
in_seq17 = array(datatrain['Year_2010'])
in_seq18 = array(datatrain['Year_2011'])
in_seq19 = array(datatrain['Year_2012'])
in_seq20 = array(datatrain['Year_2013'])
in_seq21 = array(datatrain['Year_2014'])
in_seq22 = array(datatrain['Year_2015'])
in_seq23 = array(datatrain['Year_2016'])
in_seq24 = array(datatrain['Year_2017'])
in_seq25 = array(datatrain['Year_2018'])
out_seq_train = array(datatrain['ener'])

in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))
in_seq5 = in_seq5.reshape((len(in_seq5), 1))
in_seq6 = in_seq6.reshape((len(in_seq6), 1))
in_seq7 = in_seq7.reshape((len(in_seq7), 1))
in_seq8 = in_seq8.reshape((len(in_seq8), 1))
in_seq9 = in_seq9.reshape((len(in_seq9), 1))
in_seq10 = in_seq10.reshape((len(in_seq10), 1))
in_seq11 = in_seq11.reshape((len(in_seq11), 1))
in_seq12 = in_seq12.reshape((len(in_seq12), 1))
in_seq13 = in_seq13.reshape((len(in_seq13), 1))
in_seq14 = in_seq14.reshape((len(in_seq14), 1))
in_seq15 = in_seq15.reshape((len(in_seq15), 1))
in_seq16 = in_seq16.reshape((len(in_seq16), 1))
in_seq17 = in_seq17.reshape((len(in_seq17), 1))
in_seq18 = in_seq18.reshape((len(in_seq18), 1))
in_seq19 = in_seq19.reshape((len(in_seq19), 1))
in_seq20 = in_seq20.reshape((len(in_seq20), 1))
in_seq21 = in_seq21.reshape((len(in_seq21), 1))
in_seq22 = in_seq22.reshape((len(in_seq22), 1))
in_seq23 = in_seq23.reshape((len(in_seq23), 1))
in_seq24 = in_seq18.reshape((len(in_seq18), 1))
in_seq25 = in_seq25.reshape((len(in_seq25), 1))

out_seq_train = out_seq_train.reshape((len(out_seq_train), 1))

datatrain_feed = hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, in_seq8, in_seq9, in_seq10, in_seq11, in_seq12, in_seq13, in_seq14, in_seq15, in_seq16,in_seq17, in_seq18, in_seq19, in_seq20, in_seq21, in_seq22, in_seq23, in_seq24, in_seq25, out_seq_train))



in_seq1 = array(datatest['date'])
in_seq2 = array(datatest['time'])
in_seq3 = array(datatest['Month_1'])
in_seq4 = array(datatest['Month_2'])
in_seq5 = array(datatest['Month_3'])
in_seq6 = array(datatest['Month_4'])
in_seq7 = array(datatest['Month_5'])
in_seq8 = array(datatest['Month_6'])
in_seq9 = array(datatest['Month_7'])
in_seq10 = array(datatest['Month_8'])
in_seq11 = array(datatest['Month_9'])
in_seq12 = array(datatest['Month_10'])
in_seq13 = array(datatest['Month_11'])
in_seq14 = array(datatest['Month_12'])
in_seq15 = array(datatest['Year_2008'])
in_seq16 = array(datatest['Year_2009'])
in_seq17 = array(datatest['Year_2010'])
in_seq18 = array(datatest['Year_2011'])
in_seq19 = array(datatest['Year_2012'])
in_seq20 = array(datatest['Year_2013'])
in_seq21 = array(datatest['Year_2014'])
in_seq22 = array(datatest['Year_2015'])
in_seq23 = array(datatest['Year_2016'])
in_seq24 = array(datatest['Year_2017'])
in_seq25 = array(datatest['Year_2018'])
out_seq_test = array(datatest['ener'])

in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))
in_seq5 = in_seq5.reshape((len(in_seq5), 1))
in_seq6 = in_seq6.reshape((len(in_seq6), 1))
in_seq7 = in_seq7.reshape((len(in_seq7), 1))
in_seq8 = in_seq8.reshape((len(in_seq8), 1))
in_seq9 = in_seq9.reshape((len(in_seq9), 1))
in_seq10 = in_seq10.reshape((len(in_seq10), 1))
in_seq11 = in_seq11.reshape((len(in_seq11), 1))
in_seq12 = in_seq12.reshape((len(in_seq12), 1))
in_seq13 = in_seq13.reshape((len(in_seq13), 1))
in_seq14 = in_seq14.reshape((len(in_seq14), 1))
in_seq15 = in_seq15.reshape((len(in_seq15), 1))
in_seq16 = in_seq16.reshape((len(in_seq16), 1))
in_seq17 = in_seq17.reshape((len(in_seq17), 1))
in_seq18 = in_seq18.reshape((len(in_seq18), 1))
in_seq19 = in_seq19.reshape((len(in_seq19), 1))
in_seq20 = in_seq20.reshape((len(in_seq20), 1))
in_seq21 = in_seq21.reshape((len(in_seq21), 1))
in_seq22 = in_seq22.reshape((len(in_seq22), 1))
in_seq23 = in_seq23.reshape((len(in_seq23), 1))
in_seq24 = in_seq18.reshape((len(in_seq18), 1))
in_seq25 = in_seq25.reshape((len(in_seq25), 1))

out_seq_test = out_seq_test.reshape((len(out_seq_test), 1))

datatest_feed = hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, in_seq8, in_seq9, in_seq10, in_seq11, in_seq12, in_seq13, in_seq14, in_seq15, in_seq16,in_seq17, in_seq18, in_seq19, in_seq20, in_seq21, in_seq22, in_seq23, in_seq24, in_seq25, out_seq_test))

in_seq1 = array(datahold['date'])
in_seq2 = array(datahold['time'])
in_seq3 = array(datahold['Month_1'])
in_seq4 = array(datahold['Month_2'])
in_seq5 = array(datahold['Month_3'])
in_seq6 = array(datahold['Month_4'])
in_seq7 = array(datahold['Month_5'])
in_seq8 = array(datahold['Month_6'])
in_seq9 = array(datahold['Month_7'])
in_seq10 = array(datahold['Month_8'])
in_seq11 = array(datahold['Month_9'])
in_seq12 = array(datahold['Month_10'])
in_seq13 = array(datahold['Month_11'])
in_seq14 = array(datahold['Month_12'])
in_seq15 = array(datahold['Year_2008'])
in_seq16 = array(datahold['Year_2009'])
in_seq17 = array(datahold['Year_2010'])
in_seq18 = array(datahold['Year_2011'])
in_seq19 = array(datahold['Year_2012'])
in_seq20 = array(datahold['Year_2013'])
in_seq21 = array(datahold['Year_2014'])
in_seq22 = array(datahold['Year_2015'])
in_seq23 = array(datahold['Year_2016'])
in_seq24 = array(datahold['Year_2017'])
in_seq25 = array(datahold['Year_2018'])
out_seq_hold = array(datahold['ener'])

in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))
in_seq5 = in_seq5.reshape((len(in_seq5), 1))
in_seq6 = in_seq6.reshape((len(in_seq6), 1))
in_seq7 = in_seq7.reshape((len(in_seq7), 1))
in_seq8 = in_seq8.reshape((len(in_seq8), 1))
in_seq9 = in_seq9.reshape((len(in_seq9), 1))
in_seq10 = in_seq10.reshape((len(in_seq10), 1))
in_seq11 = in_seq11.reshape((len(in_seq11), 1))
in_seq12 = in_seq12.reshape((len(in_seq12), 1))
in_seq13 = in_seq13.reshape((len(in_seq13), 1))
in_seq14 = in_seq14.reshape((len(in_seq14), 1))
in_seq15 = in_seq15.reshape((len(in_seq15), 1))
in_seq16 = in_seq16.reshape((len(in_seq16), 1))
in_seq17 = in_seq17.reshape((len(in_seq17), 1))
in_seq18 = in_seq18.reshape((len(in_seq18), 1))
in_seq19 = in_seq19.reshape((len(in_seq19), 1))
in_seq20 = in_seq20.reshape((len(in_seq20), 1))
in_seq21 = in_seq21.reshape((len(in_seq21), 1))
in_seq22 = in_seq22.reshape((len(in_seq22), 1))
in_seq23 = in_seq23.reshape((len(in_seq23), 1))
in_seq24 = in_seq18.reshape((len(in_seq18), 1))
in_seq25 = in_seq25.reshape((len(in_seq25), 1))

out_seq_hold = out_seq_test.reshape((len(out_seq_hold), 1))

datahold_feed = hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, in_seq8, in_seq9, in_seq10, in_seq11, in_seq12, in_seq13, in_seq14, in_seq15, in_seq16, out_seq_hold))

n_features = datatrain_feed.shape[1]
n_input = 10
generator_train = TimeseriesGenerator(datatrain_feed, out_seq_train, length=n_input, batch_size=len(datatrain_feed))

generator_test = TimeseriesGenerator(datatest_feed, out_seq_hold, length=n_input, batch_size=1)

generator_hold = TimeseriesGenerator(datahold_feed, out_seq_hold, length=n_input, batch_size=1)

print("timesteps, features:", n_input, n_features)

model = Sequential()

model.add(SimpleRNN(4, activation='relu', input_shape=(n_input, n_features), return_sequences = False))
model.add(Dense(1, activation='relu'))

adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='mse')

model.summary()

score = model.fit_generator(generator_train, epochs=3000, verbose=0, validation_data=generator_test)

df_result = pd.DataFrame({'Actual' : [], 'Prediction' : []})

for i in range(len(generator_test)):
    x, y = generator_test[i]
    x_input = array(x).reshape((1, n_input, n_features))
    yhat = model.predict(x_input, verbose=2)
    df_result = df_result.append({'Actual': scaler.inverse_transform(y)[0][0], 'Prediction': scaler.inverse_transform(yhat)[0][0]}, ignore_index=True)

df_result['Diff'] = 100 * (df_result['Prediction'] - df_result['Actual']) / df_result['Actual']

df_result

mean = df_result['Actual'].mean()
mae = (df_result['Actual'] - df_result['Prediction']).abs().mean()

print("mean: ", mean)
print("mae:", mae)
print("mae/mean ratio: ", 100*mae/mean,"%")
print("correctness: ", 100 - 100*mae/mean,"%")

